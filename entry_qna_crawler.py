import requests
from bs4 import BeautifulSoup
import json
import time
import re
from urllib.parse import urljoin, urlparse
import random
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

class EntryQnACrawler:
    def __init__(self, use_selenium=True):
        self.base_url = "https://playentry.org"
        self.qna_url = "https://playentry.org/community/qna"
        self.use_selenium = use_selenium
        
        if use_selenium:
            self.setup_selenium()
        else:
            self.setup_requests()
        
        self.collected_qna = []
        
    def setup_selenium(self):
        """Selenium WebDriver ì„¤ì •"""
        chrome_options = Options()
        chrome_options.add_argument('--headless')  # ë¸Œë¼ìš°ì € ì°½ ìˆ¨ê¹€
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            print("Selenium WebDriver ì„¤ì • ì™„ë£Œ")
        except Exception as e:
            print(f"Selenium ì„¤ì • ì‹¤íŒ¨: {e}")
            print("requests ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            self.use_selenium = False
            self.setup_requests()
    
    def setup_requests(self):
        """requests ì„¸ì…˜ ì„¤ì •"""
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
    
    def get_page_source(self, url, wait_time=3):
        """í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
        try:
            if self.use_selenium:
                self.driver.get(url)
                time.sleep(wait_time)
                
                # ë™ì  ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸°
                try:
                    WebDriverWait(self.driver, 10).until(
                        EC.presence_of_element_located((By.CLASS_NAME, "qna-item"))
                    )
                except:
                    # QnA ì•„ì´í…œì´ ì—†ìœ¼ë©´ ì¼ë°˜ ì½˜í…ì¸  ëŒ€ê¸°
                    time.sleep(2)
                
                return self.driver.page_source
            else:
                response = self.session.get(url, timeout=10)
                response.raise_for_status()
                return response.text
                
        except Exception as e:
            print(f"í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨ {url}: {e}")
            return None
    
    def extract_qna_list_page(self, page_num=1):
        """Q&A ëª©ë¡ í˜ì´ì§€ì—ì„œ ê°œë³„ Q&A ë§í¬ ì¶”ì¶œ"""
        list_url = f"{self.qna_url}/list/{page_num}"
        print(f"Q&A ëª©ë¡ í˜ì´ì§€ í¬ë¡¤ë§: {list_url}")
        
        page_source = self.get_page_source(list_url)
        if not page_source:
            return []
        
        soup = BeautifulSoup(page_source, 'html.parser')
        qna_links = []
        
        # Q&A ê²Œì‹œê¸€ ë§í¬ ì°¾ê¸° (ì‹¤ì œ HTML êµ¬ì¡°ì— ë”°ë¼ ì¡°ì • í•„ìš”)
        possible_selectors = [
            'a[href*="/community/qna/"]',
            '.qna-item a',
            '.list-item a',
            '.post-title a',
            'a[href*="/qna/"]'
        ]
        
        for selector in possible_selectors:
            links = soup.select(selector)
            if links:
                for link in links:
                    href = link.get('href')
                    if href and '/community/qna/' in href and '/list/' not in href:
                        full_url = urljoin(self.base_url, href)
                        title = link.get_text().strip()
                        if title and len(title) > 3:  # ì˜ë¯¸ìˆëŠ” ì œëª©ë§Œ
                            qna_links.append({
                                'url': full_url,
                                'title': title
                            })
                break
        
        # ì¤‘ë³µ ì œê±°
        seen_urls = set()
        unique_links = []
        for link in qna_links:
            if link['url'] not in seen_urls:
                seen_urls.add(link['url'])
                unique_links.append(link)
        
        print(f"  -> {len(unique_links)}ê°œì˜ Q&A ë§í¬ ë°œê²¬")
        return unique_links
    
    def extract_qna_content(self, qna_url, title):
        """ê°œë³„ Q&A í˜ì´ì§€ì—ì„œ ì§ˆë¬¸ê³¼ ë‹µë³€ ì¶”ì¶œ"""
        print(f"Q&A ë‚´ìš© ì¶”ì¶œ: {qna_url}")
        
        page_source = self.get_page_source(qna_url)
        if not page_source:
            return None
        
        soup = BeautifulSoup(page_source, 'html.parser')
        
        try:
            # ì§ˆë¬¸ ë‚´ìš© ì¶”ì¶œ
            question_selectors = [
                '.question-content',
                '.post-content',
                '.qna-question',
                '.content-body',
                '.question-text',
                '[class*="question"]',
                '[class*="content"]'
            ]
            
            question_content = None
            for selector in question_selectors:
                element = soup.select_one(selector)
                if element:
                    question_content = element.get_text().strip()
                    if len(question_content) > 10:
                        break
            
            # ì§ˆë¬¸ ë‚´ìš©ì´ ì—†ìœ¼ë©´ title ì‚¬ìš©
            if not question_content or len(question_content) < 10:
                question_content = title
            
            # ë‹µë³€ ë‚´ìš© ì¶”ì¶œ
            answer_selectors = [
                '.answer-content',
                '.reply-content',
                '.comment-content',
                '.answer-text',
                '[class*="answer"]',
                '[class*="reply"]',
                '[class*="comment"]'
            ]
            
            answers = []
            for selector in answer_selectors:
                elements = soup.select(selector)
                for element in elements:
                    answer_text = element.get_text().strip()
                    if answer_text and len(answer_text) > 5:
                        # HTML íƒœê·¸ ë° ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
                        answer_text = re.sub(r'\s+', ' ', answer_text)
                        answers.append(answer_text)
            
            # íƒœê·¸ ì¶”ì¶œ (ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ìš©)
            tags = []
            tag_selectors = [
                '.tag',
                '.label',
                '.category',
                '[class*="tag"]'
            ]
            
            for selector in tag_selectors:
                tag_elements = soup.select(selector)
                for tag_elem in tag_elements:
                    tag_text = tag_elem.get_text().strip()
                    if tag_text and len(tag_text) < 20:
                        tags.append(tag_text)
            
            return {
                'url': qna_url,
                'title': title,
                'question': question_content,
                'answers': answers,
                'tags': tags,
                'category': self.categorize_qna(title, question_content, tags)
            }
            
        except Exception as e:
            print(f"Q&A ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨ {qna_url}: {e}")
            return None
    
    def categorize_qna(self, title, question, tags):
        """Q&Aë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜"""
        text = f"{title} {question}".lower()
        
        if any(keyword in text for keyword in ['ë¸”ë¡', 'block']):
            return 'ë¸”ë¡_ì‚¬ìš©ë²•'
        elif any(keyword in text for keyword in ['ai', 'ì¸ê³µì§€ëŠ¥', 'ìŒì„±', 'ì–¼êµ´', 'ì¸ì‹']):
            return 'AI_ê¸°ëŠ¥'
        elif any(keyword in text for keyword in ['ê²Œì„', 'ë§Œë“¤ê¸°', 'í”„ë¡œì íŠ¸']):
            return 'í”„ë¡œì íŠ¸_ì œì‘'
        elif any(keyword in text for keyword in ['ì˜¤ë¥˜', 'ì—ëŸ¬', 'ì•ˆë¼', 'ì‘ë™', 'ë¬¸ì œ']):
            return 'ë¬¸ì œ_í•´ê²°'
        elif any(keyword in text for keyword in ['ìŠ¤í”„ë¼ì´íŠ¸', 'ìºë¦­í„°', 'ì˜¤ë¸Œì íŠ¸']):
            return 'ìŠ¤í”„ë¼ì´íŠ¸_ê´€ë¦¬'
        elif any(keyword in text for keyword in ['ì†Œë¦¬', 'ìŒì•…', 'ì‚¬ìš´ë“œ']):
            return 'ì†Œë¦¬_ê¸°ëŠ¥'
        elif any(keyword in text for keyword in ['ë³€ìˆ˜', 'ë¦¬ìŠ¤íŠ¸', 'ë°ì´í„°']):
            return 'ë°ì´í„°_ê´€ë¦¬'
        elif any(keyword in text for keyword in ['ê³µìœ ', 'ì—…ë¡œë“œ', 'ì €ì¥']):
            return 'ì‘í’ˆ_ê´€ë¦¬'
        else:
            return 'ê¸°íƒ€'
    
    def convert_to_training_format(self, qna_data):
        """í¬ë¡¤ë§ëœ Q&Aë¥¼ í•™ìŠµ ë°ì´í„° í˜•íƒœë¡œ ë³€í™˜"""
        training_data = []
        
        for qna in qna_data:
            if not qna['answers']:  # ë‹µë³€ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                continue
            
            # ê°€ì¥ ì¢‹ì€ ë‹µë³€ ì„ íƒ (ì²« ë²ˆì§¸ ë‹µë³€ ë˜ëŠ” ê°€ì¥ ê¸´ ë‹µë³€)
            best_answer = qna['answers'][0]
            for answer in qna['answers']:
                if len(answer) > len(best_answer) and len(answer) < 1000:
                    best_answer = answer
            
            # ì§ˆë¬¸ ì •ë¦¬
            question = qna['question']
            if len(question) > 500:
                question = question[:500] + "..."
            
            # ë‹µë³€ ì •ë¦¬
            if len(best_answer) > 800:
                best_answer = best_answer[:800] + "..."
            
            training_data.append({
                "messages": [
                    {"role": "system", "content": "ì—”íŠ¸ë¦¬ ë¸”ë¡ ì½”ë”©ì„ ë„ì™€ì£¼ëŠ” ì „ë¬¸ ì±—ë´‡ì…ë‹ˆë‹¤. ì»¤ë®¤ë‹ˆí‹°ì˜ ì‹¤ì œ ì§ˆë¬¸ì— ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤."},
                    {"role": "user", "content": question},
                    {"role": "assistant", "content": best_answer}
                ],
                "category": qna['category'],
                "source": "entry_community_qna",
                "original_url": qna['url'],
                "tags": qna['tags']
            })
        
        return training_data
    
    def crawl_qna_pages(self, max_pages=10, delay=2):
        """ì—¬ëŸ¬ í˜ì´ì§€ì˜ Q&A í¬ë¡¤ë§"""
        print(f"ì—”íŠ¸ë¦¬ Q&A ì»¤ë®¤ë‹ˆí‹° í¬ë¡¤ë§ ì‹œì‘ (ìµœëŒ€ {max_pages}í˜ì´ì§€)")
        
        all_qna_data = []
        
        for page in range(1, max_pages + 1):
            print(f"\n--- í˜ì´ì§€ {page}/{max_pages} ---")
            
            # Q&A ëª©ë¡ì—ì„œ ë§í¬ ìˆ˜ì§‘
            qna_links = self.extract_qna_list_page(page)
            
            if not qna_links:
                print(f"í˜ì´ì§€ {page}ì—ì„œ Q&A ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            # ê° Q&A ë‚´ìš© í¬ë¡¤ë§
            for i, link in enumerate(qna_links[:10]):  # í˜ì´ì§€ë‹¹ ìµœëŒ€ 10ê°œë§Œ
                print(f"  ì§„í–‰ë¥ : {i+1}/{len(qna_links[:10])}")
                
                qna_content = self.extract_qna_content(link['url'], link['title'])
                if qna_content:
                    all_qna_data.append(qna_content)
                    self.collected_qna.append(qna_content)
                
                # ì„œë²„ ë¶€í•˜ ë°©ì§€
                time.sleep(delay)
            
            # í˜ì´ì§€ ê°„ ë”œë ˆì´
            time.sleep(delay * 2)
        
        print(f"\nì´ {len(all_qna_data)}ê°œì˜ Q&A ìˆ˜ì§‘ ì™„ë£Œ")
        return all_qna_data
    
    def save_crawled_data(self, filename_prefix="entry_qna"):
        """ìˆ˜ì§‘ëœ ë°ì´í„° ì €ì¥"""
        # ì›ë³¸ ë°ì´í„° ì €ì¥
        with open(f"{filename_prefix}_raw.json", 'w', encoding='utf-8') as f:
            json.dump(self.collected_qna, f, ensure_ascii=False, indent=2)
        
        # í•™ìŠµ ë°ì´í„° í˜•íƒœë¡œ ë³€í™˜
        training_data = self.convert_to_training_format(self.collected_qna)
        
        with open(f"{filename_prefix}_training.json", 'w', encoding='utf-8') as f:
            json.dump(training_data, f, ensure_ascii=False, indent=2)
        
        # JSONL í˜•íƒœë¡œ ì €ì¥ (OpenAI fine-tuningìš©)
        with open(f"{filename_prefix}_finetuning.jsonl", 'w', encoding='utf-8') as f:
            for item in training_data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        # í†µê³„ ì¶œë ¥
        category_stats = {}
        for item in training_data:
            category = item['category']
            category_stats[category] = category_stats.get(category, 0) + 1
        
        print(f"\n=== ë°ì´í„° ì €ì¥ ì™„ë£Œ ===")
        print(f"ğŸ“ {filename_prefix}_raw.json - ì›ë³¸ Q&A ë°ì´í„°")
        print(f"ğŸ“ {filename_prefix}_training.json - í•™ìŠµìš© ë°ì´í„°")
        print(f"ğŸ“ {filename_prefix}_finetuning.jsonl - OpenAI Fine-tuningìš©")
        print(f"ğŸ“Š ì´ {len(training_data)}ê°œì˜ í•™ìŠµ ë°ì´í„° ìƒì„±")
        
        print("\nì¹´í…Œê³ ë¦¬ë³„ í†µê³„:")
        for category, count in category_stats.items():
            print(f"- {category}: {count}ê°œ")
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.use_selenium and hasattr(self, 'driver'):
            self.driver.quit()

def main():
    # Selenium ì‚¬ìš© ì—¬ë¶€ (Chrome ë“œë¼ì´ë²„ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•¨)
    use_selenium = True  # Falseë¡œ ì„¤ì •í•˜ë©´ requestsë§Œ ì‚¬ìš©
    
    crawler = EntryQnACrawler(use_selenium=use_selenium)
    
    try:
        # Q&A í¬ë¡¤ë§ ì‹¤í–‰
        qna_data = crawler.crawl_qna_pages(max_pages=5, delay=1)
        
        # ë°ì´í„° ì €ì¥
        crawler.save_crawled_data("entry_community_qna")
        
        print("í¬ë¡¤ë§ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    finally:
        crawler.close()

if __name__ == "__main__":
    main()